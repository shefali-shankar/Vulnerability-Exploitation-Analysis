# -*- coding: utf-8 -*-

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

data=pd.read_csv('vuln_exploit.csv')

#displaying top 5 rows
data.head() 

#displaying columns

data.columns

#displaying bottom 5 rows
data.tail()

#checking for null values
data.isna().sum()

#dropping less correlated/no correlated values:cvi_id,qid
#dropping columns like ExploitName,ExploitID,exploit_type,exploit_found_date,exploit_type
#because these columns contains almost 90% null values

data.drop(['qid','cve_id','ExploitName','Is_Verified','ExploitID',
           'exploit_type','exploit_found_date'],axis=1,inplace=True)

data.columns

data.shape

data.describe()

#dataset information 
#shows datatype of each column

data.info()

#removing duplicate values

data.drop_duplicates(keep='first',inplace=True) #inplace: Boolean values, removes rows with duplicates if True.
data.shape

#checking for correlation between the variables by using heatmap

sns.heatmap(data.corr())
#checking with output variable

data['Exploited'].unique() #Unique values in Exploited: True, False
data['Exploited'].value_counts() #False    3306 , True      189

#Data is imbalance and it leads to overfitting problem. Here False values will be more compare to True values

colors = ["#0101DF", "#DF0101"]

sns.countplot('Exploited', data=data, palette=colors)
plt.title('Class Distributions \n (0: Not Exploited || 1: Exploited)', fontsize=14)

#Balancing dataset
#need to balance the data
#uploading resample package

from sklearn.utils import resample

y = data.Exploited
X = data.drop('Exploited', axis=1)

# setting up testing and training sets

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)

X = pd.concat([X_train, y_train], axis=1)
X.head()

not_Exploited = X[X.Exploited==False]
Exploited= X[X.Exploited==True]
######### Replacing False with True to Equalise the number of False and True values in Exploited
Exploited_upsampled = resample(Exploited,
                          replace=True, # sample with replacement
                          n_samples=len(not_Exploited), # match number in majority class
                          random_state=27) # reproducible results

# combine majority and upsampled minority
upsampled = pd.concat([not_Exploited, Exploited_upsampled])

# check new class counts
upsampled.Exploited.value_counts() #True   2478, False  2478
##########

#Now data is balanced.
#copying upsampled data to df1
df1=upsampled

#In Exploited column replacing True with 1 and False with 0.

df1['Exploited']=df1['Exploited'].map({True:1,False:0})
df1['Exploited'].value_counts()

#checking correlation of cvss_seviarity with exploited
#It is strogly correlated with output 
#high seviarity leads to high exploitation

plt.figure(figsize=(10,5))
Cvss_sevearity_status=sns.countplot(df1['cvss_severity'], hue=df1['Exploited'])
Cvss_sevearity_status

#Comparing sevearity level with output
#Sevearity level is more then network gets exploited

plt.figure(figsize=(10,5))
sevearity_level_status=sns.countplot(df1['severity_level'], hue=df1['Exploited'])
sevearity_level_status

#Comparing total_vulns with output parameter
# Total_vulns also affects the output
#If vulnerability increases then exploitation also increases

plt.figure(figsize=(30,5))
total_vulns_status=sns.countplot(df1['total_vulns'], hue=df1['Exploited'])
total_vulns_status

#comparing report_confidence with output
#cvss_report_confidence also affects the output
#for cvss_report_confidence =3 , exploitation is more.

plt.figure(figsize=(10,5))
cvss_report_confidence_status=sns.countplot(df1['cvss.report_confidence'], hue=df1['Exploited'])
cvss_report_confidence_status

#comparing category status with output
#category windows and internet explorer,office application,cisco exploited more
plt.figure(figsize=(30,5))
category_status=sns.countplot(df1['category'], hue=df1['Exploited'])
category_status 

#Comparing vendor with exploit
# Microsoft has high exploitation comapre others

plt.figure(figsize=(20,5))
vendor_status=sns.countplot(df1['vendor'], hue=df1['Exploited'])
vendor_status

#comparing type with output
#type also affect the output

plt.figure(figsize=(5,5))
type_status=sns.countplot(df1['type'], hue=df1['Exploited'])
type_status 

#Comapring tag with output
#Here security,oracle critical,code execution  tags have high number of exploitation.
plt.figure(figsize=(30,5))
tag_status=sns.countplot(df1['tag'], hue=df1['Exploited'])
tag_status

#Assigning input variables to X and output variables to y

X=df1.drop(['Exploited'],axis=1)
y=df1['Exploited']

X.shape
y.shape

#Converting categorical variables to numerics by using
#Label encoder

from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()
columns=['type','cvss_severity','title','cvss_vector_string','category','cvss.vector_string','vendor','tag']
for column in columns:
    X[column]=labelencoder.fit_transform(X[column])

print(X[column])
# model based ranking by using RandomForestClassifier and cross value score

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

clf = RandomForestClassifier(n_estimators = 50, max_depth = 4) #no. of trees, max levels in each decision tree

scores = []
num_features = len(X.columns)
for i in range(num_features):
    col = X.columns[i]
    score = np.mean(cross_val_score(clf, X[col].values.reshape(-1,1), y, cv=10))
    scores.append((int(score*100), col))

s1=sorted(scores, reverse = True)
s1

#Selecting 10 best features based on their scores
print("\n The 10 best features selected by this method are :")
for i in range(10):
    print(s1[i])

#selecting 5 worst features based on their scores
print ("\n The 5 worst features selected by this method are :")
for i in range(5):
    print(s1[len(scores)-1-i])

#method2: Assgning rank by using L1 Regularization and Logistic Regression
#Finding best parameter by using L1 Regualarization

from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
# {'logisticregression__C': [1, 10, 100, 1000]
param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100]}
pipe = make_pipeline(StandardScaler(), LogisticRegression(penalty = 'l2'))     
grid = GridSearchCV(pipe, param_grid, cv = 10)
grid.fit(X, y)
print(grid.best_params_)

#Best parameter

print(grid.best_params_)

X_scaled = StandardScaler().fit_transform(X)
clf = LogisticRegression(penalty = 'l2', C = 100)
clf.fit(X_scaled,y)

# L1 regularization forces  some Features 0 coefficient  
zero_feat = []
nonzero_feat = []
# type(clf.coef_)
for i in range(num_features):
    coef = clf.coef_[0,i]
    if coef == 0:
        zero_feat.append(X.columns[i])
    else:
        nonzero_feat.append((coef, X.columns[i]))
        
print ('Features that have coeffcient of 0 are: ', zero_feat)

# features which have non zero coefficient

print ('\n Features that have non-zero coefficients are:')
print (sorted(nonzero_feat, reverse = True))

s2=sorted(nonzero_feat,reverse=True)
s2

# Selection top 10 features based on their rank
print('\n Best 10 features by L2 Regularization method is \n')
for i in range(10):
    print(s2[i])


#splitting dataset into training and testing dataset.
#Here we taken 25% as testing dataset and 75% as training dataset

from sklearn.preprocessing import  StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


pipeline = Pipeline([
    ('normalizer', StandardScaler()), #Step1 - normalize data
    ('clf', LogisticRegression()) #step2 - classifier
])
pipeline.steps

X_train, X_test, y_train, y_test = train_test_split(X,
                                                   y,
                                                   test_size = 0.3,
                                                   random_state = 10)

from sklearn.model_selection import cross_validate

scores = cross_validate(pipeline, X_train, y_train)
scores

scores['test_score'].mean()

#####################Pipeline

from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

clfs = []
clfs.append(LogisticRegression())
clfs.append(SVC())
clfs.append(KNeighborsClassifier(n_neighbors=3))
clfs.append(DecisionTreeClassifier())
clfs.append(RandomForestClassifier())
clfs.append(GradientBoostingClassifier())

for classifier in clfs:
    pipeline.set_params(clf = classifier)
    scores = cross_validate(pipeline, X_train, y_train)
    print('---------------------------------')
    print(str(classifier))
    print('-----------------------------------')
    for key, values in scores.items():
            print(key,' mean ', values.mean())
            print(key,' std ', values.std())


# from randomforest classifier we got more test_score mean that is 93%

from sklearn.ensemble import RandomForestClassifier
classifier2=RandomForestClassifier()
classifier2.fit(X_train,y_train)

import pickle
model_columns = list(X.columns)
pickle.dump(model_columns, open('model_columns.pkl','wb'))
pickle.dump(classifier2, open('model.pkl','wb')) #write bytes
model = pickle.load(open('model.pkl','rb')) #read bytes
print(model.predict(X_test))

y_pred2=classifier2.predict(X_test)
print(y_pred2)
#Summarize the result by using confusion matrix

from sklearn.metrics import confusion_matrix
cm2=confusion_matrix(y_test,y_pred2)
cm2

#finding accuracy of Random Forest classifer
#we achived accuracy arround 92% by using random forest classifier

#from sklearn.metrics import accuracy_score
print(accuracy_score(y_test,y_pred2))

##################

#Principal component Analysis

#finding correlation
correlation = df1.corr()
plt.figure(figsize=(10,10))
sns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='viridis')
plt.title('Correlation between different fearures')

#Applying standardscaler convert everything to within +ve and -ve range
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X=scaler.fit_transform(X)


#Applying Principal component analysis to get ranking of maultivariate

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
X=sc.fit_transform(X)

from sklearn.decomposition import PCA
pca = PCA()
pca.fit_transform(X)

pca.get_covariance()

explained_variance=pca.explained_variance_ratio_
explained_variance

with plt.style.context('dark_background'):
    plt.figure(figsize=(6, 4))

    plt.bar(range(24), explained_variance, alpha=0.5, align='center',
            label='individual explained variance')
    plt.ylabel('Explained variance ratio')
    plt.xlabel('Principal components')
    plt.legend(loc='best')
    plt.tight_layout()

pca.components_
#15 features has strong correlation correlation and considering n_components as 15
pca=PCA(n_components=15)
X_new=pca.fit_transform(X)
X_new

pca.get_covariance()

explained_variance=pca.explained_variance_ratio_
explained_variance

with plt.style.context('dark_background'):
    plt.figure(figsize=(6, 4))

    plt.bar(range(15), explained_variance, alpha=0.5, align='center',
            label='individual explained variance')
    plt.ylabel('Explained variance ratio')
    plt.xlabel('Principal components')
    plt.legend(loc='best')
    plt.tight_layout()

from sklearn.model_selection import train_test_split
X_train1, X_test1, y_train1, y_test1 = train_test_split(X_new, y, test_size=0.2, random_state=1)
X_train1.shape
y_train1.shape

#Train the model by using support vector classifier

from sklearn.svm import SVC
SVC_pca = SVC()
SVC_pca.fit(X_train1,y_train1)

print("Train score after PCA",SVC_pca.score(X_train1,y_train1),"%")
print("Test score after PCA",SVC_pca.score(X_test1,y_test1),"%")

y_pred3=SVC_pca.predict(X_test1)

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test1,y_pred3)

from sklearn.metrics import accuracy_score
accuracy_score(y_test1,y_pred3)

### from svc achived test_score  arround 90%

#train the model by randomforest classfier by considering default estimator

from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
model.fit(X_train1,y_train1)
print("Train score after PCA",model.score(X_train1,y_train1),"%")
print("Test score after PCA",model.score(X_test1,y_test1),"%")


y_pred4=model.predict(X_test1)
confusion_matrix(y_test1,y_pred4)

accuracy_score(y_test1,y_pred3)




















